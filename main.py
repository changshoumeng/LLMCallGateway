#!/usr/bin/env python3
"""
LLMCallGateway - ä¸“ä¸šLLM APIç½‘å…³æœåŠ¡

åŸºäºLiteLLMæ„å»ºçš„å¤šæ¨¡å‹ç»Ÿä¸€ç½‘å…³æœåŠ¡ï¼Œæ”¯æŒï¼š
- å°†æ‰€æœ‰æ¨¡å‹è¯·æ±‚ç»Ÿä¸€ä¸ºOpenAIæ ¼å¼
- è¯¦ç»†çš„LLMäº¤äº’æ—¥å¿—è·Ÿè¸ª
- å®Œæ•´çš„tokenç»Ÿè®¡å’Œæ€§èƒ½ç›‘æ§
- æ¨¡å—åŒ–æ¶æ„ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
"""

import time
from contextlib import asynccontextmanager
from typing import Dict, Any, Optional

from fastapi import FastAPI, HTTPException, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# å¯¼å…¥åº”ç”¨æ¨¡å—
from app.core.config import settings
from app.core.logging import system_logger
from app.models.api_models import (
    ChatCompletionRequest, ModelList, Model, HealthResponse, MetricsResponse,
    EmbeddingRequest, EmbeddingResponse
)
from app.services.llm_service import llm_service
from app.services.metrics import metrics_collector
from app.utils.helpers import extract_user_id_from_request, create_error_response


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    system_logger.info(f"ğŸš€ å¯åŠ¨ {settings.app_name} v{settings.app_version}")
    system_logger.info(f"ğŸ“Š è°ƒè¯•æ¨¡å¼: {settings.debug}")
    system_logger.info(f"ğŸŒ æœåŠ¡åœ°å€: http://{settings.host}:{settings.port}")
    system_logger.info(f"ğŸ“š APIæ–‡æ¡£: http://{settings.host}:{settings.port}/docs")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    system_logger.info(f"â¹ï¸ {settings.app_name} æœåŠ¡æ­£åœ¨å…³é—­...")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title=settings.app_name,
    description="ä¸“ä¸šLLM APIç½‘å…³æœåŠ¡ - ç»Ÿä¸€å¤šæ¨¡å‹ä¸ºOpenAIæ ¼å¼ï¼Œæä¾›è¯¦ç»†çš„äº¤äº’æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§",
    version=settings.app_version,
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
if settings.enable_cors:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )


@app.middleware("http")
async def request_logging_middleware(request: Request, call_next):
    """è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶"""
    start_time = time.time()
    
    # è®°å½•è¯·æ±‚ä¿¡æ¯
    user_id = extract_user_id_from_request(request)
    system_logger.info(
        f"ğŸ“¥ {request.method} {request.url.path} | "
        f"User: {user_id or 'Anonymous'} | "
        f"IP: {request.client.host if request.client else 'Unknown'}"
    )
    
    # å¤„ç†è¯·æ±‚
    response = await call_next(request)
    
    # è®°å½•å“åº”ä¿¡æ¯
    process_time = time.time() - start_time
    system_logger.info(
        f"ğŸ“¤ {request.method} {request.url.path} | "
        f"Status: {response.status_code} | "
        f"Time: {process_time:.3f}s"
    )
    
    # æ·»åŠ å“åº”å¤´
    response.headers["X-Process-Time"] = str(process_time)
    response.headers["X-Service-Version"] = settings.app_version
    
    return response


# === è¾…åŠ©å‡½æ•° ===

async def preprocess_embedding_data(raw_data: dict) -> dict:
    """
    é¢„å¤„ç†embeddingsè¾“å…¥æ•°æ®ï¼Œæ”¯æŒè‡ªåŠ¨tokenè§£ç 
    """
    import json

    processed_data = raw_data.copy()

    if "input" in processed_data:
        input_data = processed_data["input"]

        # å¤„ç†å•ä¸ªtokenæ•°ç»„
        if isinstance(input_data, list) and len(input_data) > 0:
            if all(isinstance(x, int) for x in input_data):
                # å°è¯•è§£ç tokenæ•°ç»„
                decoded_text = try_decode_tokens(input_data)
                if decoded_text:
                    processed_data["input"] = decoded_text
                    system_logger.info(f"âœ… è‡ªåŠ¨è§£ç tokenæ•°ç»„ä¸ºæ–‡æœ¬: '{decoded_text[:50]}...'")
                else:
                    raise ValueError(
                        f"æ£€æµ‹åˆ°tokenizedæ•°å­—æ•°ç»„ä½†æ— æ³•è§£ç ã€‚è¯·å‘é€åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²è€Œä¸æ˜¯tokenæ•°ç»„ã€‚"
                        f"\næ­£ç¡®æ ¼å¼: 'åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²'"
                        f"\né”™è¯¯æ ¼å¼: {input_data[:10]}..."
                    )
            # å¤„ç†åŒ…å«tokenæ•°ç»„çš„åˆ—è¡¨
            elif any(isinstance(item, list) and all(isinstance(x, int) for x in item) for item in input_data if isinstance(item, list)):
                processed_list = []
                for item in input_data:
                    if isinstance(item, list) and all(isinstance(x, int) for x in item):
                        decoded_text = try_decode_tokens(item)
                        if decoded_text:
                            processed_list.append(decoded_text)
                        else:
                            raise ValueError(f"æ— æ³•è§£ç tokenæ•°ç»„: {item[:10]}...")
                    else:
                        processed_list.append(item)
                processed_data["input"] = processed_list
                system_logger.info(f"âœ… è‡ªåŠ¨è§£ç åˆ—è¡¨ä¸­çš„tokenæ•°ç»„")

    return processed_data


def try_decode_tokens(input_data) -> Optional[str]:
    """
    å°è¯•å°†tokenizedæ•°ç»„è§£ç ä¸ºæ–‡æœ¬
    """
    try:
        import tiktoken

        if isinstance(input_data, list) and len(input_data) > 0:
            if all(isinstance(x, int) for x in input_data):
                # å°è¯•ä½¿ç”¨ä¸åŒçš„ç¼–ç å™¨è§£ç 
                encoders = ["cl100k_base", "gpt2", "r50k_base", "p50k_base"]

                for encoder_name in encoders:
                    try:
                        encoding = tiktoken.get_encoding(encoder_name)
                        decoded_text = encoding.decode(input_data)
                        if decoded_text and len(decoded_text.strip()) > 0:
                            system_logger.info(f"ğŸ”„ ä½¿ç”¨{encoder_name}è§£ç : {len(input_data)} tokens -> æ–‡æœ¬")
                            return decoded_text
                    except Exception:
                        continue

                # å¦‚æœæ‰€æœ‰ç¼–ç å™¨éƒ½å¤±è´¥ï¼Œè¿”å›None
                system_logger.warning(f"âš ï¸ æ— æ³•è§£ç tokenæ•°ç»„: {input_data[:10]}...")
                return None

        return None
    except ImportError:
        system_logger.warning("âš ï¸ tiktokenåº“æœªå®‰è£…ï¼Œæ— æ³•è§£ç tokenæ•°ç»„")
        return None
    except Exception as e:
        system_logger.error(f"è§£ç tokenæ•°ç»„æ—¶å‡ºé”™: {e}")
        return None


# === APIè·¯ç”±å®šä¹‰ ===

@app.get("/", response_model=HealthResponse)
async def root():
    """å¥åº·æ£€æŸ¥å’ŒæœåŠ¡ä¿¡æ¯"""
    return HealthResponse(
        service=settings.app_name,
        status="running",
        version=settings.app_version,
        description="LLM APIç½‘å…³æœåŠ¡ - ç»Ÿä¸€å¤šæ¨¡å‹ä¸ºOpenAIæ ¼å¼"
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """è¯¦ç»†å¥åº·æ£€æŸ¥"""
    return HealthResponse(
        service=settings.app_name,
        status="healthy",
        version=settings.app_version,
        description="æ‰€æœ‰ç³»ç»Ÿæ­£å¸¸è¿è¡Œ"
    )


@app.get("/v1/models", response_model=ModelList)
async def list_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        available_models = llm_service.get_available_models()
        
        models = [
            Model(
                id=model_id,
                created=int(time.time()),
                owned_by="llmcallgateway"
            )
            for model_id in available_models
        ]
        
        system_logger.info(f"ğŸ“‹ è¿”å›æ¨¡å‹åˆ—è¡¨: {len(models)} ä¸ªæ¨¡å‹")
        return ModelList(data=models)
    
    except Exception as e:
        system_logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        raise create_error_response("è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥", "models_error", 500)


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest, http_request: Request):
    """åˆ›å»ºèŠå¤©è¡¥å…¨"""
    try:
        # æå–ç”¨æˆ·ID
        user_id = extract_user_id_from_request(http_request)
        
        # åŸºæœ¬éªŒè¯
        if not request.messages:
            raise create_error_response("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º", "invalid_request", 400)
        
        if not request.model:
            raise create_error_response("æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º", "invalid_request", 400)
        
        # è°ƒç”¨LLMæœåŠ¡
        result = await llm_service.create_chat_completion(request, user_id)
        
        # æ ¹æ®å“åº”ç±»å‹è¿”å›
        if request.stream:
            return StreamingResponse(
                result,
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/event-stream"
                }
            )
        else:
            return result
    
    except HTTPException:
        raise
    except Exception as e:
        system_logger.error(f"èŠå¤©è¡¥å…¨å¤„ç†å¤±è´¥: {e}")
        raise create_error_response(f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}", "completion_error", 500)


@app.post("/v1/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(http_request: Request):
    """åˆ›å»ºæ–‡æœ¬åµŒå…¥ - æ”¯æŒè‡ªåŠ¨tokenè§£ç """
    try:
        # æå–ç”¨æˆ·ID
        user_id = extract_user_id_from_request(http_request)

        # è·å–åŸå§‹JSONæ•°æ®
        raw_data = await http_request.json()

        system_logger.info(f"åŸå§‹JSONæ•°æ®: {raw_data}")

        # æ™ºèƒ½é¢„å¤„ç†è¾“å…¥æ•°æ®
        processed_data = await preprocess_embedding_data(raw_data)

        # åˆ›å»ºéªŒè¯åçš„è¯·æ±‚å¯¹è±¡
        try:
            request = EmbeddingRequest(**processed_data)
        except Exception as e:
            system_logger.error(f"è¯·æ±‚éªŒè¯å¤±è´¥: {e}")
            raise create_error_response(f"è¯·æ±‚æ ¼å¼é”™è¯¯: {str(e)}", "invalid_request", 400)

        # åŸºæœ¬éªŒè¯
        if not request.input:
            raise create_error_response("è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º", "invalid_request", 400)

        if not request.model:
            raise create_error_response("æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º", "invalid_request", 400)

        # è°ƒç”¨LLMæœåŠ¡
        result = await llm_service.create_embeddings(request, user_id)

        return result

    except HTTPException:
        raise
    except Exception as e:
        system_logger.error(f"åµŒå…¥ç”Ÿæˆå¤„ç†å¤±è´¥: {e}")
        raise create_error_response(f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}", "embedding_error", 500)


@app.get("/metrics", response_model=Dict[str, Any])
async def get_metrics():
    """è·å–æœåŠ¡æŒ‡æ ‡"""
    try:
        stats = metrics_collector.get_current_stats()
        return stats
    except Exception as e:
        system_logger.error(f"è·å–æŒ‡æ ‡å¤±è´¥: {e}")
        raise create_error_response("è·å–æŒ‡æ ‡å¤±è´¥", "metrics_error", 500)


@app.get("/metrics/models")
async def get_model_metrics():
    """è·å–æŒ‰æ¨¡å‹åˆ†ç»„çš„æŒ‡æ ‡"""
    try:
        model_stats = metrics_collector.get_model_stats()
        return model_stats
    except Exception as e:
        system_logger.error(f"è·å–æ¨¡å‹æŒ‡æ ‡å¤±è´¥: {e}")
        raise create_error_response("è·å–æ¨¡å‹æŒ‡æ ‡å¤±è´¥", "metrics_error", 500)


@app.get("/metrics/trends")
async def get_metrics_trends(hours: int = 24):
    """è·å–æŒ‡æ ‡è¶‹åŠ¿æ•°æ®"""
    try:
        if hours < 1 or hours > 168:  # æœ€å¤š7å¤©
            hours = 24
        
        trends = metrics_collector.get_hourly_trends(hours)
        return trends
    except Exception as e:
        system_logger.error(f"è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
        raise create_error_response("è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥", "metrics_error", 500)


@app.post("/admin/reset-metrics")
async def reset_metrics():
    """é‡ç½®æŒ‡æ ‡æ•°æ®ï¼ˆç®¡ç†å‘˜åŠŸèƒ½ï¼‰"""
    try:
        metrics_collector.reset_stats()
        system_logger.info("ğŸ“Š æŒ‡æ ‡æ•°æ®å·²é‡ç½®")
        return {"message": "æŒ‡æ ‡æ•°æ®å·²é‡ç½®", "timestamp": int(time.time())}
    except Exception as e:
        system_logger.error(f"é‡ç½®æŒ‡æ ‡å¤±è´¥: {e}")
        raise create_error_response("é‡ç½®æŒ‡æ ‡å¤±è´¥", "admin_error", 500)


# === é”™è¯¯å¤„ç† ===

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """è¾“å…¥éªŒè¯å¼‚å¸¸å¤„ç†å™¨ - ä¸“é—¨å¤„ç†422é”™è¯¯"""

    def is_tokenized_input_error(errors):
        """æ£€æµ‹æ˜¯å¦æ˜¯tokenizedè¾“å…¥é”™è¯¯"""
        for error in errors:
            if ('input' in error.get('loc', []) and
                error.get('type') == 'string_type' and
                isinstance(error.get('input'), list) and
                len(error.get('input', [])) > 0 and
                all(isinstance(x, int) for x in error.get('input', [])[:10])):  # æ£€æŸ¥å‰10ä¸ªå…ƒç´ 
                return True
        return False

    # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå…¥æ¥å£çš„é”™è¯¯
    is_embedding_request = request.url.path == "/v1/embeddings"
    errors = exc.errors()

    if is_embedding_request and is_tokenized_input_error(errors):
        # é’ˆå¯¹tokenizedè¾“å…¥æä¾›ä¸“é—¨çš„é”™è¯¯ä¿¡æ¯
        return JSONResponse(
            status_code=422,
            content={
                "error": {
                    "message": "è¾“å…¥æ ¼å¼é”™è¯¯ï¼šæ£€æµ‹åˆ°tokenizedæ•°å­—æ•°ç»„",
                    "type": "invalid_request_error",
                    "code": "invalid_input_format",
                    "details": "embeddings APIéœ€è¦åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œä¸æ¥å—tokenizedçš„æ•°å­—æ•°ç»„ã€‚",
                    "correct_examples": {
                        "single_text": '{"input": "Hello world", "model": "text-embedding-3-small"}',
                        "multiple_texts": '{"input": ["Hello", "World"], "model": "text-embedding-3-small"}'
                    },
                    "common_mistakes": [
                        "âŒ ä¸è¦å‘é€: {\"input\": [3134, 419, 57086], ...}",
                        "âŒ ä¸è¦å‘é€: {\"input\": [[3134, 419]], ...}",
                        "âœ… åº”è¯¥å‘é€: {\"input\": \"åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²\", ...}"
                    ]
                }
            }
        )

    # é€šç”¨éªŒè¯é”™è¯¯å¤„ç†
    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": "è¯·æ±‚æ ¼å¼éªŒè¯å¤±è´¥",
                "type": "validation_error",
                "details": [
                    {
                        "loc": error["loc"],
                        "msg": error["msg"],
                        "type": error["type"]
                    }
                    for error in errors
                ]
            }
        }
    )


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTPå¼‚å¸¸å¤„ç†å™¨"""
    return JSONResponse(
        status_code=exc.status_code,
        content=exc.detail if isinstance(exc.detail, dict) else {"error": {"message": exc.detail}}
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """é€šç”¨å¼‚å¸¸å¤„ç†å™¨"""
    system_logger.error(f"æœªå¤„ç†çš„å¼‚å¸¸: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
                "type": "internal_error",
                "code": 500
            }
        }
    )


# === åº”ç”¨å¯åŠ¨ ===

def main():
    """ä¸»å‡½æ•°"""
    system_logger.info(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
    system_logger.info(f"   Host: {settings.host}")
    system_logger.info(f"   Port: {settings.port}")
    system_logger.info(f"   Debug: {settings.debug}")
    system_logger.info(f"   Log Level: {settings.log_level}")
    system_logger.info(f"   Reload: {settings.reload}")
    
    if settings.reload:
        system_logger.info("ğŸ”„ çƒ­é‡è½½æ¨¡å¼å¯ç”¨")
        uvicorn.run(
            "main:app",
            host=settings.host,
            port=settings.port,
            reload=True,
            reload_excludes=["logs/*", "*.log"],
            log_level=settings.log_level.lower()
        )
    else:
        system_logger.info("âš¡ ç”Ÿäº§æ¨¡å¼å¯åŠ¨")
        uvicorn.run(
            "main:app",
            host=settings.host,
            port=settings.port,
            reload=False,
            log_level=settings.log_level.lower()
        )


if __name__ == "__main__":
    main()