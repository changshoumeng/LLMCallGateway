#!/usr/bin/env python3
"""
LLMCallGateway - ä¸“ä¸šLLM APIç½‘å…³æœåŠ¡

åŸºäºLiteLLMæ„å»ºçš„å¤šæ¨¡å‹ç»Ÿä¸€ç½‘å…³æœåŠ¡ï¼Œæ”¯æŒï¼š
- å°†æ‰€æœ‰æ¨¡å‹è¯·æ±‚ç»Ÿä¸€ä¸ºOpenAIæ ¼å¼
- è¯¦ç»†çš„LLMäº¤äº’æ—¥å¿—è·Ÿè¸ª
- å®Œæ•´çš„tokenç»Ÿè®¡å’Œæ€§èƒ½ç›‘æ§
- æ¨¡å—åŒ–æ¶æ„ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
"""

import time
from contextlib import asynccontextmanager
from typing import Dict, Any

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# å¯¼å…¥åº”ç”¨æ¨¡å—
from app.core.config import settings
from app.core.logging import system_logger
from app.models.api_models import (
    ChatCompletionRequest, ModelList, Model, HealthResponse, MetricsResponse
)
from app.services.llm_service import llm_service
from app.services.metrics import metrics_collector
from app.utils.helpers import extract_user_id_from_request, create_error_response


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    system_logger.info(f"ğŸš€ å¯åŠ¨ {settings.app_name} v{settings.app_version}")
    system_logger.info(f"ğŸ“Š è°ƒè¯•æ¨¡å¼: {settings.debug}")
    system_logger.info(f"ğŸŒ æœåŠ¡åœ°å€: http://{settings.host}:{settings.port}")
    system_logger.info(f"ğŸ“š APIæ–‡æ¡£: http://{settings.host}:{settings.port}/docs")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    system_logger.info(f"â¹ï¸ {settings.app_name} æœåŠ¡æ­£åœ¨å…³é—­...")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title=settings.app_name,
    description="ä¸“ä¸šLLM APIç½‘å…³æœåŠ¡ - ç»Ÿä¸€å¤šæ¨¡å‹ä¸ºOpenAIæ ¼å¼ï¼Œæä¾›è¯¦ç»†çš„äº¤äº’æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§",
    version=settings.app_version,
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
if settings.enable_cors:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )


@app.middleware("http")
async def request_logging_middleware(request: Request, call_next):
    """è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶"""
    start_time = time.time()
    
    # è®°å½•è¯·æ±‚ä¿¡æ¯
    user_id = extract_user_id_from_request(request)
    system_logger.info(
        f"ğŸ“¥ {request.method} {request.url.path} | "
        f"User: {user_id or 'Anonymous'} | "
        f"IP: {request.client.host if request.client else 'Unknown'}"
    )
    
    # å¤„ç†è¯·æ±‚
    response = await call_next(request)
    
    # è®°å½•å“åº”ä¿¡æ¯
    process_time = time.time() - start_time
    system_logger.info(
        f"ğŸ“¤ {request.method} {request.url.path} | "
        f"Status: {response.status_code} | "
        f"Time: {process_time:.3f}s"
    )
    
    # æ·»åŠ å“åº”å¤´
    response.headers["X-Process-Time"] = str(process_time)
    response.headers["X-Service-Version"] = settings.app_version
    
    return response


# === APIè·¯ç”±å®šä¹‰ ===

@app.get("/", response_model=HealthResponse)
async def root():
    """å¥åº·æ£€æŸ¥å’ŒæœåŠ¡ä¿¡æ¯"""
    return HealthResponse(
        service=settings.app_name,
        status="running",
        version=settings.app_version,
        description="LLM APIç½‘å…³æœåŠ¡ - ç»Ÿä¸€å¤šæ¨¡å‹ä¸ºOpenAIæ ¼å¼"
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """è¯¦ç»†å¥åº·æ£€æŸ¥"""
    return HealthResponse(
        service=settings.app_name,
        status="healthy",
        version=settings.app_version,
        description="æ‰€æœ‰ç³»ç»Ÿæ­£å¸¸è¿è¡Œ"
    )


@app.get("/v1/models", response_model=ModelList)
async def list_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        available_models = llm_service.get_available_models()
        
        models = [
            Model(
                id=model_id,
                created=int(time.time()),
                owned_by="llmcallgateway"
            )
            for model_id in available_models
        ]
        
        system_logger.info(f"ğŸ“‹ è¿”å›æ¨¡å‹åˆ—è¡¨: {len(models)} ä¸ªæ¨¡å‹")
        return ModelList(data=models)
    
    except Exception as e:
        system_logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        raise create_error_response("è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥", "models_error", 500)


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest, http_request: Request):
    """åˆ›å»ºèŠå¤©è¡¥å…¨"""
    try:
        # æå–ç”¨æˆ·ID
        user_id = extract_user_id_from_request(http_request)
        
        # åŸºæœ¬éªŒè¯
        if not request.messages:
            raise create_error_response("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º", "invalid_request", 400)
        
        if not request.model:
            raise create_error_response("æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º", "invalid_request", 400)
        
        # è°ƒç”¨LLMæœåŠ¡
        result = await llm_service.create_chat_completion(request, user_id)
        
        # æ ¹æ®å“åº”ç±»å‹è¿”å›
        if request.stream:
            return StreamingResponse(
                result,
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/event-stream"
                }
            )
        else:
            return result
    
    except HTTPException:
        raise
    except Exception as e:
        system_logger.error(f"èŠå¤©è¡¥å…¨å¤„ç†å¤±è´¥: {e}")
        raise create_error_response(f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}", "completion_error", 500)


@app.get("/metrics", response_model=Dict[str, Any])
async def get_metrics():
    """è·å–æœåŠ¡æŒ‡æ ‡"""
    try:
        stats = metrics_collector.get_current_stats()
        return stats
    except Exception as e:
        system_logger.error(f"è·å–æŒ‡æ ‡å¤±è´¥: {e}")
        raise create_error_response("è·å–æŒ‡æ ‡å¤±è´¥", "metrics_error", 500)


@app.get("/metrics/models")
async def get_model_metrics():
    """è·å–æŒ‰æ¨¡å‹åˆ†ç»„çš„æŒ‡æ ‡"""
    try:
        model_stats = metrics_collector.get_model_stats()
        return model_stats
    except Exception as e:
        system_logger.error(f"è·å–æ¨¡å‹æŒ‡æ ‡å¤±è´¥: {e}")
        raise create_error_response("è·å–æ¨¡å‹æŒ‡æ ‡å¤±è´¥", "metrics_error", 500)


@app.get("/metrics/trends")
async def get_metrics_trends(hours: int = 24):
    """è·å–æŒ‡æ ‡è¶‹åŠ¿æ•°æ®"""
    try:
        if hours < 1 or hours > 168:  # æœ€å¤š7å¤©
            hours = 24
        
        trends = metrics_collector.get_hourly_trends(hours)
        return trends
    except Exception as e:
        system_logger.error(f"è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
        raise create_error_response("è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥", "metrics_error", 500)


@app.post("/admin/reset-metrics")
async def reset_metrics():
    """é‡ç½®æŒ‡æ ‡æ•°æ®ï¼ˆç®¡ç†å‘˜åŠŸèƒ½ï¼‰"""
    try:
        metrics_collector.reset_stats()
        system_logger.info("ğŸ“Š æŒ‡æ ‡æ•°æ®å·²é‡ç½®")
        return {"message": "æŒ‡æ ‡æ•°æ®å·²é‡ç½®", "timestamp": int(time.time())}
    except Exception as e:
        system_logger.error(f"é‡ç½®æŒ‡æ ‡å¤±è´¥: {e}")
        raise create_error_response("é‡ç½®æŒ‡æ ‡å¤±è´¥", "admin_error", 500)


# === é”™è¯¯å¤„ç† ===

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTPå¼‚å¸¸å¤„ç†å™¨"""
    return JSONResponse(
        status_code=exc.status_code,
        content=exc.detail if isinstance(exc.detail, dict) else {"error": {"message": exc.detail}}
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """é€šç”¨å¼‚å¸¸å¤„ç†å™¨"""
    system_logger.error(f"æœªå¤„ç†çš„å¼‚å¸¸: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
                "type": "internal_error",
                "code": 500
            }
        }
    )


# === åº”ç”¨å¯åŠ¨ ===

def main():
    """ä¸»å‡½æ•°"""
    system_logger.info(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
    system_logger.info(f"   Host: {settings.host}")
    system_logger.info(f"   Port: {settings.port}")
    system_logger.info(f"   Debug: {settings.debug}")
    system_logger.info(f"   Log Level: {settings.log_level}")
    system_logger.info(f"   Reload: {settings.reload}")
    
    if settings.reload:
        system_logger.info("ğŸ”„ çƒ­é‡è½½æ¨¡å¼å¯ç”¨")
        uvicorn.run(
            "main:app",
            host=settings.host,
            port=settings.port,
            reload=True,
            reload_excludes=["logs/*", "*.log"],
            log_level=settings.log_level.lower()
        )
    else:
        system_logger.info("âš¡ ç”Ÿäº§æ¨¡å¼å¯åŠ¨")
        uvicorn.run(
            "main:app",
            host=settings.host,
            port=settings.port,
            reload=False,
            log_level=settings.log_level.lower()
        )


if __name__ == "__main__":
    main()