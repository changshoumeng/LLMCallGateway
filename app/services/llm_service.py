"""
LLMä»£ç†æœåŠ¡
åŸºäºLiteLLMå®ç°å¤šæ¨¡å‹ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒè¯¦ç»†çš„äº¤äº’æ—¥å¿—å’ŒæŒ‡æ ‡ç»Ÿè®¡
"""

import uuid
import time
import asyncio
from typing import Dict, List, Any, Optional, AsyncGenerator, Union, Tuple
import json

import litellm
from litellm import completion, acompletion, embedding, aembedding

from ..core.config import settings
from ..core.logging import system_logger, llm_interaction_logger
from ..models.api_models import (
    ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk,
    ChatMessage, ChatCompletionChoice, ChatCompletionChunkChoice,
    DeltaMessage, Usage, RequestContext,
    EmbeddingRequest, EmbeddingResponse, EmbeddingData,
    ToolCall, ToolCallFunction, ToolCallDelta
)
from ..services.metrics import metrics_collector


class LLMService:
    """LLMä»£ç†æœåŠ¡"""
    
    def __init__(self):
        self.setup_litellm()
    
    def setup_litellm(self) -> None:
        """é…ç½®LiteLLM"""
        # åŸºç¡€é…ç½®
        litellm.set_verbose = settings.debug
        litellm.drop_params = True  # è‡ªåŠ¨è¿‡æ»¤ä¸æ”¯æŒçš„å‚æ•°
        litellm.request_timeout = settings.request_timeout
        
        # å¦‚æœæœ‰é…ç½®APIå¯†é’¥ï¼Œè®¾ç½®é»˜è®¤å¯†é’¥
        if settings.litellm_api_key:
            litellm.api_key = settings.litellm_api_key
        
        # å¦‚æœæœ‰é…ç½®åŸºç¡€URLï¼Œè®¾ç½®é»˜è®¤URL
        if settings.litellm_base_url:
            litellm.api_base = settings.litellm_base_url
        
        system_logger.info(f"LiteLLMé…ç½®å®Œæˆ - Timeout: {settings.request_timeout}s")
    
    async def create_chat_completion(self, request: ChatCompletionRequest,
                                   user_id: Optional[str] = None) -> Union[ChatCompletionResponse, AsyncGenerator]:
        """åˆ›å»ºèŠå¤©è¡¥å…¨"""
        # ç”Ÿæˆè¯·æ±‚ID
        request_id = str(uuid.uuid4())[:8]
        
        # åˆ›å»ºè¯·æ±‚ä¸Šä¸‹æ–‡
        context = RequestContext(
            request_id=request_id,
            start_time=time.time(),
            model=request.model,
            user_id=user_id,
            stream=request.stream or False
        )
        
        # å¼€å§‹è®°å½•æŒ‡æ ‡
        metrics = metrics_collector.start_request(context)
        
        try:
            # å‡†å¤‡LiteLLMè¯·æ±‚å‚æ•°
            llm_request = self._prepare_litellm_request(request)
            
            # å¼€å§‹è®°å½•ä¸‹æ¸¸äº¤äº’
            llm_interaction_logger.start_interaction(
                request_id, "litellm", llm_request
            )
            
            if request.stream:
                # æµå¼å“åº”
                return self._handle_stream_completion(
                    request_id, llm_request, context, metrics
                )
            else:
                # éæµå¼å“åº”
                return await self._handle_non_stream_completion(
                    request_id, llm_request, context, metrics
                )
        
        except Exception as e:
            # è®°å½•é”™è¯¯äº¤äº’
            llm_interaction_logger.log_error_interaction(request_id, e, "create_chat_completion")
            
            # å®ŒæˆæŒ‡æ ‡è®°å½•
            metrics_collector.complete_request(
                request_id, success=False, error_message=str(e)
            )
            
            system_logger.error(f"èŠå¤©è¡¥å…¨å¤±è´¥: {e}")
            raise e
    
    def _normalize_tool_call_entry(self, tool_call: Any) -> Optional[Dict[str, Any]]:
        """å°†å·¥å…·è°ƒç”¨å¯¹è±¡æ ‡å‡†åŒ–ä¸ºdict"""
        try:
            if hasattr(tool_call, "model_dump"):
                data = tool_call.model_dump(exclude_none=True)
            elif isinstance(tool_call, dict):
                data = {k: v for k, v in tool_call.items() if v is not None}
            else:
                data = {
                    "id": getattr(tool_call, "id", None),
                    "type": getattr(tool_call, "type", None),
                }
                func = getattr(tool_call, "function", None)
                if func is not None:
                    if hasattr(func, "model_dump"):
                        data["function"] = func.model_dump(exclude_none=True)
                    elif isinstance(func, dict):
                        data["function"] = {k: v for k, v in func.items() if v is not None}
                    else:
                        data["function"] = {
                            "name": getattr(func, "name", None),
                            "arguments": getattr(func, "arguments", None)
                        }
                elif isinstance(tool_call, dict) and isinstance(tool_call.get("function"), dict):
                    data["function"] = tool_call["function"]
            call_id = data.get("id")
            func_info = data.get("function", {})
            func_name = func_info.get("name")
            func_args = func_info.get("arguments")
            if call_id and func_name is not None:
                if func_args is None:
                    func_info["arguments"] = ""
                else:
                    func_info["arguments"] = str(func_args)
                data["function"] = func_info
                data.setdefault("type", "function")
                return {
                    "id": str(call_id),
                    "type": str(data.get("type", "function")),
                    "function": {
                        "name": str(func_info.get("name")),
                        "arguments": str(func_info.get("arguments", ""))
                    }
                }
        except Exception:
            return None
        return None

    def _extract_tool_calls(
        self, message: Any
    ) -> Tuple[Optional[List[ToolCall]], Optional[List[Dict[str, Any]]]]:
        """è§£æå·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œè¿”å›æ¨¡å‹å¯¹è±¡å’Œæ—¥å¿—ä¸“ç”¨æ•°æ®"""
        tool_calls_attr = getattr(message, "tool_calls", None)
        if not tool_calls_attr:
            return None, None

        tool_call_models: List[ToolCall] = []
        log_tool_calls: List[Dict[str, Any]] = []

        for tool_call in tool_calls_attr:
            normalized = self._normalize_tool_call_entry(tool_call)
            if not normalized:
                continue
            log_tool_calls.append(normalized)
            try:
                tool_call_models.append(
                    ToolCall(
                        id=normalized["id"],
                        type=normalized["type"],
                        function=ToolCallFunction(
                            name=normalized["function"]["name"],
                            arguments=normalized["function"]["arguments"]
                        )
                    )
                )
            except Exception:
                continue

        return (
            tool_call_models if tool_call_models else None,
            log_tool_calls if log_tool_calls else None
        )

    def _extract_function_call_payload(self, message: Any) -> Optional[Dict[str, Any]]:
        """è§£ææ—§ç‰ˆfunction_callå­—æ®µ"""
        fc = getattr(message, "function_call", None)
        if not fc:
            return None
        if hasattr(fc, "model_dump"):
            payload = fc.model_dump(exclude_none=True)
        elif isinstance(fc, dict):
            payload = {k: v for k, v in fc.items() if v is not None}
        else:
            payload = {
                "name": getattr(fc, "name", None),
                "arguments": getattr(fc, "arguments", None)
            }
        if not payload.get("name"):
            return None
        if payload.get("arguments") is None:
            payload["arguments"] = ""
        else:
            payload["arguments"] = str(payload["arguments"])
        return payload

    def _normalize_message_content(self, content: Optional[Union[str, List[Dict[str, Any]]]]) -> str:
        """å°†æ¶ˆæ¯å†…å®¹ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå…¼å®¹å¤šæ¨¡æ€å†…å®¹å—"""
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            fragments: List[str] = []
            for block in content:
                if isinstance(block, dict):
                    # ä¼˜å…ˆæå–æ–‡æœ¬å­—æ®µ
                    if "text" in block and block["text"] is not None:
                        fragments.append(str(block["text"]))
                    elif block.get("type") in {"input_text", "output_text"} and block.get("text") is not None:
                        fragments.append(str(block["text"]))
                    elif block.get("type") == "image_url" and isinstance(block.get("image_url"), dict):
                        image_url = block["image_url"].get("url")
                        if image_url:
                            fragments.append(f"[image:{image_url}]")
                    else:
                        try:
                            fragments.append(json.dumps(block, ensure_ascii=False))
                        except TypeError:
                            fragments.append(str(block))
                else:
                    fragments.append(str(block))
            return "\n".join([frag for frag in fragments if frag]).strip()
        return str(content)
    
    def _extract_user_query(self, messages: List[ChatMessage]) -> str:
        """æå–ç”¨æˆ·æŸ¥è¯¢å†…å®¹"""
        user_messages: List[str] = []
        for msg in messages:
            if msg.role != "user":
                continue
            normalized = self._normalize_message_content(msg.content)
            if normalized:
                user_messages.append(normalized)
        if user_messages:
            return user_messages[-1]  # è¿”å›æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        return "æ— ç”¨æˆ·æŸ¥è¯¢"
    
    def _prepare_litellm_request(self, request: ChatCompletionRequest) -> Dict[str, Any]:
        """
        å‡†å¤‡å‘é€ç»™LiteLLMçš„è¯·æ±‚å‚æ•°ã€‚

        å°†ChatCompletionRequestå¯¹è±¡è½¬æ¢ä¸ºä¸‹æ¸¸å…¼å®¹æ ¼å¼ï¼Œå¹¶ä¿ç•™OpenAI APIçš„æ‰©å±•å­—æ®µï¼Œå¦‚
        å·¥å…·å®šä¹‰ï¼ˆtools/functionsï¼‰ä»¥åŠè°ƒç”¨ç­–ç•¥ï¼ˆtool_choice/function_callï¼‰ã€‚
        """
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼: roleã€contentã€name
        messages: List[Dict[str, Any]] = []
        for msg in request.messages:
            m: Dict[str, Any] = {"role": msg.role}
            # å¤„ç†contentå­—æ®µï¼šå…è®¸å­—ç¬¦ä¸²æˆ–å†…å®¹å—åˆ—è¡¨
            if msg.content is not None:
                m["content"] = msg.content
            elif msg.tool_calls or msg.function_call:
                # å½“å­˜åœ¨å·¥å…·/å‡½æ•°è°ƒç”¨æ—¶å…è®¸contentä¸ºNoneä»¥éµå¾ªOpenAIè§„èŒƒ
                m["content"] = None
            else:
                m["content"] = ""
            if msg.name:
                m["name"] = msg.name
            if msg.tool_call_id:
                m["tool_call_id"] = msg.tool_call_id
            if msg.function_call:
                m["function_call"] = msg.function_call
            if msg.tool_calls:
                m["tool_calls"] = [
                    tool_call.model_dump(exclude_none=True)
                    if hasattr(tool_call, "model_dump")
                    else tool_call  # å…è®¸ç›´æ¥ä¼ å…¥å­—å…¸
                    for tool_call in msg.tool_calls
                ]
            messages.append(m)

        llm_request: Dict[str, Any] = {
            "model": request.model,
            "messages": messages,
            "stream": bool(request.stream),
        }

        # é€šç”¨å¯é€‰å‚æ•°
        optional_params = [
            "temperature", "max_tokens", "top_p", "frequency_penalty",
            "presence_penalty", "stop", "n", "user"
        ]
        for param in optional_params:
            value = getattr(request, param, None)
            if value is not None:
                llm_request[param] = value

        # å¤„ç†å·¥å…·/å‡½æ•°å®šä¹‰
        if request.tools is not None:
            llm_request["tools"] = request.tools
        elif request.functions is not None:
            # å…¼å®¹æ—§ç‰ˆå‡½æ•°å®šä¹‰
            llm_request["functions"] = request.functions

        # å¤„ç†è°ƒç”¨ç­–ç•¥
        if request.tool_choice is not None:
            llm_request["tool_choice"] = request.tool_choice
        elif request.function_call is not None:
            llm_request["function_call"] = request.function_call

        return llm_request
    
    async def _handle_non_stream_completion(
        self, request_id: str, llm_request: Dict[str, Any],
        context: RequestContext, metrics
    ) -> ChatCompletionResponse:
        """å¤„ç†éæµå¼è¡¥å…¨"""
        
        try:
            # è®°å½•ä¸‹æ¸¸è¯·æ±‚æ—¶é—´
            downstream_start = time.time()
            
            # è°ƒç”¨LiteLLM
            response = await acompletion(**llm_request)
            
            downstream_time = time.time() - downstream_start
            
            # è®°å½•ä¸‹æ¸¸å“åº”
            response_data = {
                "id": response.id,
                "model": response.model,
                "choices": [
                    {
                        "index": choice.index,
                        "message": {
                            "role": choice.message.role,
                            "content": choice.message.content
                        },
                        "finish_reason": choice.finish_reason
                    }
                    for choice in response.choices
                ],
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                }
            }
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„å“åº”æ ¼å¼
            converted_choices: List[ChatCompletionChoice] = []
            for list_index, choice in enumerate(response.choices):
                # è§£æå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
                tool_calls_list, log_tool_calls = self._extract_tool_calls(choice.message)
                function_call_payload = self._extract_function_call_payload(choice.message)

                # æ„å»ºæ—¥å¿—ç”¨message
                message_payload: Dict[str, Any] = {
                    "role": choice.message.role,
                    "content": choice.message.content
                }
                if getattr(choice.message, "tool_call_id", None):
                    message_payload["tool_call_id"] = choice.message.tool_call_id
                if log_tool_calls:
                    message_payload["tool_calls"] = log_tool_calls
                if function_call_payload:
                    message_payload["function_call"] = function_call_payload

                # æ›´æ–°æ—¥å¿—æ•°æ®choices
                if list_index < len(response_data["choices"]):
                    response_data["choices"][list_index]["message"] = message_payload

                # æ„å»ºChatMessage
                msg_content = choice.message.content if choice.message.content is not None else None
                tool_call_id = getattr(choice.message, "tool_call_id", None)
                chat_msg = ChatMessage(
                    role=choice.message.role,
                    content=msg_content,
                    tool_call_id=tool_call_id,
                    tool_calls=tool_calls_list,
                    function_call=function_call_payload
                )
                converted_choices.append(
                    ChatCompletionChoice(
                        index=choice.index,
                        message=chat_msg,
                        finish_reason=choice.finish_reason
                    )
                )

            choices = converted_choices
            
            usage = None
            if response.usage:
                usage = Usage(
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                )

            # å®Œæˆäº¤äº’è®°å½•ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼‰
            llm_interaction_logger.complete_interaction(
                request_id, response_data, downstream_time, success=True
            )

            # è·å–tokenä½¿ç”¨æƒ…å†µç”¨äºæŒ‡æ ‡
            prompt_tokens = response.usage.prompt_tokens if response.usage else 0
            completion_tokens = response.usage.completion_tokens if response.usage else 0
            
            # å®ŒæˆæŒ‡æ ‡è®°å½•
            metrics_collector.complete_request(
                request_id,
                success=True,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens
            )
            
            return ChatCompletionResponse(
                id=response.id,
                model=response.model,
                choices=choices,
                usage=usage
            )
        
        except Exception as e:
            llm_interaction_logger.log_error_interaction(request_id, e, "non_stream_completion")
            raise e
    
    async def _handle_stream_completion(
        self, request_id: str, llm_request: Dict[str, Any],
        context: RequestContext, metrics
    ) -> AsyncGenerator[str, None]:
        """å¤„ç†æµå¼è¡¥å…¨"""
        
        accumulated_content = ""
        chunk_count = 0
        finish_reason = None
        prompt_tokens = 0
        completion_tokens = 0
        tool_call_accumulator: Dict[str, Dict[str, Any]] = {}
        function_call_accumulator: Optional[Dict[str, Any]] = None
        
        try:
            # è®°å½•ä¸‹æ¸¸è¯·æ±‚æ—¶é—´
            downstream_start = time.time()
            
            # è°ƒç”¨LiteLLMæµå¼API
            response_stream = await acompletion(**llm_request)
            
            async for chunk in response_stream:
                chunk_count += 1
                
                if chunk.choices:
                    choice = chunk.choices[0]
                    
                    # ç´¯ç§¯å†…å®¹
                    if choice.delta.content:
                        accumulated_content += choice.delta.content
                        
                        # æµå¼å—è®°å½•å·²é›†æˆåˆ°æœ€ç»ˆå“åº”ä¸­
                    
                    # è®°å½•å®ŒæˆåŸå› 
                    if choice.finish_reason:
                        finish_reason = choice.finish_reason
                    
                    # æ„å»ºå“åº”å—
                    delta = DeltaMessage()
                    # è®¾ç½®è§’è‰²
                    if getattr(choice.delta, "role", None):
                        delta.role = choice.delta.role
                    # è®¾ç½®å†…å®¹
                    if getattr(choice.delta, "content", None):
                        delta.content = choice.delta.content
                    # è§£æå·¥å…·è°ƒç”¨å¢é‡
                    tool_calls_delta: Optional[List[ToolCallDelta]] = None
                    # å…¼å®¹æ—§ç‰ˆfunction_callå¢é‡
                    if getattr(choice.delta, "function_call", None):
                        fc_delta = choice.delta.function_call
                        fc_payload: Optional[Dict[str, Any]] = None
                        if hasattr(fc_delta, "model_dump"):
                            fc_payload = fc_delta.model_dump(exclude_none=True)
                        elif isinstance(fc_delta, dict):
                            fc_payload = fc_delta
                        else:
                            try:
                                fc_payload = dict(fc_delta)
                            except Exception:
                                fc_payload = None
                        if fc_payload:
                            # ç´¯ç§¯function_callä¿¡æ¯
                            if function_call_accumulator is None:
                                function_call_accumulator = {
                                    "name": fc_payload.get("name"),
                                    "arguments": ""
                                }
                            if fc_payload.get("name"):
                                function_call_accumulator["name"] = fc_payload["name"]
                            if fc_payload.get("arguments"):
                                function_call_accumulator["arguments"] = (
                                    function_call_accumulator.get("arguments", "") + fc_payload["arguments"]
                                )
                            delta.function_call = fc_payload
                    if hasattr(choice.delta, "tool_calls") and choice.delta.tool_calls:
                        tool_calls_delta = []
                        for tc in choice.delta.tool_calls:
                            try:
                                tc_id = getattr(tc, "id", None)
                                if tc_id is None and isinstance(tc, dict):
                                    tc_id = tc.get("id")
                                tc_type = getattr(tc, "type", None)
                                if tc_type is None and isinstance(tc, dict):
                                    tc_type = tc.get("type")
                                # å‡½æ•°ä¿¡æ¯
                                if hasattr(tc, "function"):
                                    func = tc.function
                                    func_name = getattr(func, "name", None)
                                    func_args = getattr(func, "arguments", None)
                                elif isinstance(tc, dict) and isinstance(tc.get("function"), dict):
                                    func_name = tc["function"].get("name")
                                    func_args = tc["function"].get("arguments")
                                else:
                                    func_name = None
                                    func_args = None
                                # æ„å»ºå¢é‡å¯¹è±¡
                                if func_name is not None and func_args is not None:
                                    tool_calls_delta.append(
                                        ToolCallDelta(
                                            id=tc_id,
                                            type=tc_type,
                                            function=ToolCallFunction(
                                                name=str(func_name), arguments=str(func_args)
                                            )
                                        )
                                    )
                                    # ç´¯ç§¯å·¥å…·è°ƒç”¨ä¿¡æ¯
                                    call_id = str(tc_id) if tc_id else None
                                    if call_id:
                                        existing = tool_call_accumulator.get(call_id, {
                                            "id": call_id,
                                            "type": str(tc_type) if tc_type else "function",
                                            "function": {"name": None, "arguments": ""}
                                        })
                                        if func_name:
                                            existing["function"]["name"] = str(func_name)
                                        if func_args:
                                            prev_args = existing["function"].get("arguments") or ""
                                            existing["function"]["arguments"] = prev_args + str(func_args)
                                        tool_call_accumulator[call_id] = existing
                            except Exception:
                                continue
                    if tool_calls_delta:
                        delta.tool_calls = tool_calls_delta
                    
                    chunk_choice = ChatCompletionChunkChoice(
                        index=choice.index,
                        delta=delta,
                        finish_reason=choice.finish_reason
                    )
                    
                    response_chunk = ChatCompletionChunk(
                        id=chunk.id,
                        model=chunk.model,
                        choices=[chunk_choice]
                    )
                    
                    yield f"data: {response_chunk.model_dump_json()}\n\n"
            
            downstream_time = time.time() - downstream_start
            
            # ä¼°ç®—tokenä½¿ç”¨ï¼ˆLiteLLMæµå¼å¯èƒ½æ²¡æœ‰usageï¼‰
            if accumulated_content:
                # ç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰æˆ– 1.5 å­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰
                completion_tokens = max(1, len(accumulated_content) // 3)
            
            # ä¼°ç®—è¾“å…¥tokenï¼ˆåŸºäºè¯·æ±‚æ¶ˆæ¯é•¿åº¦ï¼‰
            if prompt_tokens == 0:
                total_input_text = " ".join([msg["content"] for msg in llm_request["messages"] if msg.get("content")])
                prompt_tokens = max(1, len(total_input_text) // 3)  # ä½¿ç”¨ç›¸åŒçš„ä¼°ç®—æ–¹æ³•
            
            # è®°å½•ä¸‹æ¸¸å“åº”æ‘˜è¦
            response_summary = {
                "content": accumulated_content,
                "finish_reason": finish_reason,
                "chunk_count": chunk_count,
                "estimated_tokens": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": prompt_tokens + completion_tokens
                }
            }

            if tool_call_accumulator:
                response_summary["tool_calls"] = [
                    {
                        "id": call_id,
                        "type": data.get("type", "function"),
                        "function": {
                            "name": data.get("function", {}).get("name"),
                            "arguments": data.get("function", {}).get("arguments", "")
                        }
                    }
                    for call_id, data in tool_call_accumulator.items()
                ]
            if function_call_accumulator:
                response_summary["function_call"] = function_call_accumulator
            
            # å®Œæˆäº¤äº’è®°å½•
            llm_interaction_logger.complete_interaction(
                request_id, response_summary, downstream_time, success=True
            )
            
            # å®ŒæˆæŒ‡æ ‡è®°å½•
            metrics_collector.complete_request(
                request_id,
                success=True,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens
            )
            
            # å‘é€ç»“æŸæ ‡è®°
            yield "data: [DONE]\n\n"
        
        except Exception as e:
            llm_interaction_logger.log_error_interaction(request_id, e, "stream_completion")
            
            # å®ŒæˆæŒ‡æ ‡è®°å½•
            metrics_collector.complete_request(
                request_id, success=False, error_message=str(e)
            )
            
            # åœ¨æµå¼å“åº”ä¸­å‘é€é”™è¯¯
            error_chunk = {
                "error": {
                    "message": str(e),
                    "type": "stream_error"
                }
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"

    def _try_decode_tokens(self, input_data) -> Optional[str]:
        """
        å°è¯•å°†tokenizedæ•°ç»„è§£ç ä¸ºæ–‡æœ¬
        å¦‚æœè¾“å…¥æ˜¯tokenæ•°ç»„ï¼Œå°è¯•ä½¿ç”¨tiktokenè§£ç 
        """
        try:
            import tiktoken

            # æ£€æŸ¥æ˜¯å¦æ˜¯tokenæ•°ç»„
            if isinstance(input_data, list) and len(input_data) > 0:
                if all(isinstance(x, int) for x in input_data):
                    # å°è¯•ä½¿ç”¨ä¸åŒçš„ç¼–ç å™¨è§£ç 
                    encoders = ["cl100k_base", "gpt2", "r50k_base", "p50k_base"]

                    for encoder_name in encoders:
                        try:
                            encoding = tiktoken.get_encoding(encoder_name)
                            decoded_text = encoding.decode(input_data)
                            if decoded_text and len(decoded_text.strip()) > 0:
                                system_logger.info(f"ğŸ”„ è‡ªåŠ¨è§£ç tokenæ•°ç»„ ({encoder_name}): {len(input_data)} tokens -> '{decoded_text[:50]}...'")
                                return decoded_text
                        except Exception:
                            continue

                    # å¦‚æœæ‰€æœ‰ç¼–ç å™¨éƒ½å¤±è´¥ï¼Œè¿”å›None
                    system_logger.warning(f"âš ï¸ æ— æ³•è§£ç tokenæ•°ç»„: {input_data[:10]}...")
                    return None

            return None
        except ImportError:
            system_logger.warning("âš ï¸ tiktokenåº“æœªå®‰è£…ï¼Œæ— æ³•è§£ç tokenæ•°ç»„")
            return None
        except Exception as e:
            system_logger.error(f"è§£ç tokenæ•°ç»„æ—¶å‡ºé”™: {e}")
            return None

    def _preprocess_embedding_input(self, request: EmbeddingRequest) -> EmbeddingRequest:
        """
        é¢„å¤„ç†åµŒå…¥è¾“å…¥ï¼Œæ”¯æŒè‡ªåŠ¨tokenè§£ç 
        """
        processed_input = request.input

        # å¤„ç†å•ä¸ªè¾“å…¥
        if isinstance(request.input, list) and len(request.input) > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯tokenæ•°ç»„ (æ‰€æœ‰å…ƒç´ éƒ½æ˜¯int)
            if all(isinstance(x, int) for x in request.input):
                decoded_text = self._try_decode_tokens(request.input)
                if decoded_text:
                    processed_input = decoded_text
                    system_logger.info(f"âœ… æˆåŠŸè§£ç tokenizedè¾“å…¥ä¸ºæ–‡æœ¬")
                else:
                    # å¦‚æœè§£ç å¤±è´¥ï¼ŒæŠ›å‡ºå‹å¥½çš„é”™è¯¯
                    raise ValueError(
                        "æ£€æµ‹åˆ°tokenizedæ•°å­—æ•°ç»„ä½†æ— æ³•è§£ç ã€‚è¯·å‘é€åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²è€Œä¸æ˜¯tokenæ•°ç»„ã€‚"
                        f"\næ­£ç¡®æ ¼å¼: 'åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²'"
                        f"\né”™è¯¯æ ¼å¼: {request.input[:10]}..."
                    )
            # å¤„ç†åˆ—è¡¨ä¸­åŒ…å«tokenæ•°ç»„çš„æƒ…å†µ
            elif any(isinstance(item, list) and all(isinstance(x, int) for x in item) for item in request.input if isinstance(item, list)):
                processed_list = []
                for item in request.input:
                    if isinstance(item, list) and all(isinstance(x, int) for x in item):
                        decoded_text = self._try_decode_tokens(item)
                        if decoded_text:
                            processed_list.append(decoded_text)
                        else:
                            raise ValueError(f"æ— æ³•è§£ç tokenæ•°ç»„: {item[:10]}...")
                    else:
                        processed_list.append(item)
                processed_input = processed_list
                system_logger.info(f"âœ… æˆåŠŸè§£ç åˆ—è¡¨ä¸­çš„tokenizedè¾“å…¥")

        # åˆ›å»ºå¤„ç†åçš„è¯·æ±‚å¯¹è±¡
        return EmbeddingRequest(
            input=processed_input,
            model=request.model,
            encoding_format=request.encoding_format,
            dimensions=request.dimensions,
            user=request.user
        )

    async def create_embeddings(self, request: EmbeddingRequest,
                               user_id: Optional[str] = None) -> EmbeddingResponse:
        """åˆ›å»ºæ–‡æœ¬åµŒå…¥"""
        # é¢„å¤„ç†è¾“å…¥ï¼Œæ”¯æŒè‡ªåŠ¨tokenè§£ç 
        try:
            request = self._preprocess_embedding_input(request)
        except ValueError as e:
            system_logger.error(f"è¾“å…¥é¢„å¤„ç†å¤±è´¥: {e}")
            raise ValueError(str(e))

        # ç”Ÿæˆè¯·æ±‚ID
        request_id = str(uuid.uuid4())[:8]

        # åˆ›å»ºè¯·æ±‚ä¸Šä¸‹æ–‡
        context = RequestContext(
            request_id=request_id,
            start_time=time.time(),
            model=request.model,
            user_id=user_id,
            stream=False,  # embeddingsä¸æ”¯æŒæµå¼
            request_type="embedding"
        )

        # å¼€å§‹è®°å½•æŒ‡æ ‡
        metrics = metrics_collector.start_request(context)

        try:
            # å‡†å¤‡LiteLLMè¯·æ±‚å‚æ•°
            llm_request = self._prepare_embedding_request(request)

            # å¼€å§‹è®°å½•ä¸‹æ¸¸äº¤äº’
            llm_interaction_logger.start_interaction(
                request_id, "litellm", llm_request
            )

            # å¤„ç†åµŒå…¥è¯·æ±‚
            return await self._handle_embedding_request(
                request_id, llm_request, context, metrics
            )

        except Exception as e:
            # è®°å½•é”™è¯¯äº¤äº’
            llm_interaction_logger.log_error_interaction(request_id, e, "create_embeddings")

            # å®ŒæˆæŒ‡æ ‡è®°å½•
            metrics_collector.complete_request(
                request_id, success=False, error_message=str(e)
            )

            system_logger.error(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            raise e

    def _prepare_embedding_request(self, request: EmbeddingRequest) -> Dict[str, Any]:
        """å‡†å¤‡åµŒå…¥è¯·æ±‚å‚æ•°"""
        llm_request = {
            "model": request.model,
            "input": request.input,
        }

        # æ·»åŠ å¯é€‰å‚æ•°
        if request.user:
            llm_request["user"] = request.user

        if request.dimensions:
            llm_request["dimensions"] = request.dimensions

        if request.encoding_format:
            llm_request["encoding_format"] = request.encoding_format

        return llm_request

    async def _handle_embedding_request(
        self, request_id: str, llm_request: Dict[str, Any],
        context: RequestContext, metrics
    ) -> EmbeddingResponse:
        """å¤„ç†åµŒå…¥è¯·æ±‚"""

        try:
            # è®°å½•ä¸‹æ¸¸è¯·æ±‚æ—¶é—´
            downstream_start = time.time()

            # è°ƒç”¨LiteLLMåµŒå…¥API
            response = await aembedding(**llm_request)

            downstream_time = time.time() - downstream_start

            # å¤„ç†LiteLLMå“åº” - æ ¹æ®å®é™…æµ‹è¯•ï¼ŒLiteLLMè¿”å›çš„æ˜¯å¯¹è±¡æ ¼å¼
            # ä½† data å­—æ®µåŒ…å«çš„æ˜¯å­—å…¸åˆ—è¡¨ï¼Œä¸æ˜¯å¯¹è±¡åˆ—è¡¨
            response_data = {
                "object": "list",
                "data": [
                    {
                        "object": "embedding",
                        "embedding": data["embedding"],  # dataæ˜¯å­—å…¸ï¼Œä½¿ç”¨å­—å…¸è®¿é—®
                        "index": data["index"]
                    }
                    for data in response.data
                ],
                "model": response.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                }
            }
            model_name = response.model
            usage_info = response.usage if response.usage else {}

            # å®Œæˆäº¤äº’è®°å½•
            llm_interaction_logger.complete_interaction(
                request_id, response_data, downstream_time, success=True
            )

            # è·å–tokenä½¿ç”¨æƒ…å†µç”¨äºæŒ‡æ ‡
            prompt_tokens = usage_info.get("prompt_tokens", 0)
            total_tokens = usage_info.get("total_tokens", 0)

            # å®ŒæˆæŒ‡æ ‡è®°å½•
            metrics_collector.complete_request(
                request_id,
                success=True,
                prompt_tokens=prompt_tokens,
                completion_tokens=0,  # embeddingsæ²¡æœ‰completion tokens
                total_tokens=total_tokens
            )

            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„å“åº”æ ¼å¼
            # LiteLLMè¿”å›å¯¹è±¡æ ¼å¼ï¼Œä½†dataæ˜¯å­—å…¸åˆ—è¡¨
            embedding_data = [
                EmbeddingData(
                    embedding=data["embedding"],  # dataæ˜¯å­—å…¸ï¼Œä½¿ç”¨å­—å…¸è®¿é—®
                    index=data["index"]
                )
                for data in response.data
            ]

            usage = Usage(
                prompt_tokens=prompt_tokens,
                completion_tokens=0,  # embeddingsæ²¡æœ‰completion tokens
                total_tokens=total_tokens
            )

            return EmbeddingResponse(
                data=embedding_data,
                model=model_name,
                usage=usage
            )

        except Exception as e:
            llm_interaction_logger.log_error_interaction(request_id, e, "embedding_request")
            raise e

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        # LiteLLMæ”¯æŒçš„å¸¸è§æ¨¡å‹
        common_models = [
            # OpenAI
            "gpt-4o-mini", "gpt-4o", "gpt-4-turbo", "gpt-4", "gpt-3.5-turbo",
            # Anthropic
            "claude-3-sonnet-20240229", "claude-3-haiku-20240307",
            # Google
            "gemini-pro", "gemini-pro-vision",
            # Mistral
            "mistral-small", "mistral-medium", "mistral-large",
            # å…¶ä»–
            "command-nightly", "llama-2-70b-chat"
        ]
        
        return common_models


# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()
